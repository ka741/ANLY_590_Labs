{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf-and-keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_SjPxONCeUY"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnjjUBk2USFz"
      },
      "source": [
        "#### first steps in TensorFlow\n",
        "\n",
        "TensorFlow is a framework for automatic differentiation. It has an interface very similar to that of NumPy operating on array data. Let's start by rewriting our NumPy functions for a single neural net layer and a simple two-layer neural net in TensorFlow.\n",
        "\n",
        "A few notes:\n",
        "1. Instead of using `np.dot` for matrix multiplication, check out the `tf.matmul` function instead.\n",
        "2. TensorFlow already has built-in functions for all of the common neural net activation functions with the exception of the \"linear\" activation function, which in TensorFlow is often thought of as having no activation function (since $f(x)=x$ does not change the input). We show you where to find these activation functions in the package and provide a simple implementation of the linear activation function.\n",
        "3. notice that we are using a slightly different function signature for the neural network that lets us pack all of the parameters (weights and biases) into a list, as this will be convenient later when it comes to fitting the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9Rg-tq8CkvB"
      },
      "source": [
        "# activation functions\n",
        "tf.nn.sigmoid\n",
        "tf.nn.tanh\n",
        "tf.nn.relu\n",
        "linear = lambda x: x\n",
        "\n",
        "def nn_layer(x, w, b, f):\n",
        "  '''\n",
        "  transformation for a single layer of a neural net\n",
        "\n",
        "  x: array on inputs, shape: (batch size, input dim)\n",
        "  w: array of weights, shape: (input dim, output dim)\n",
        "  b: array of biases, shape: (output dim)\n",
        "  f: activation function\n",
        "  '''\n",
        "  pass\n",
        "\n",
        "def nn_2layers(x, params, f_hidden, f_out):\n",
        "  '''\n",
        "  transformation for simple 2 layer neural net (hidden + output layers)\n",
        "\n",
        "  x: array of inputs\n",
        "  params: list of parameter arrays: [w_1, b_1, w_2, b_2]\n",
        "  f_hidden: activation function on the hidden layer\n",
        "  f_out: output function on the output layer\n",
        "  '''\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhThmdLRY-3_"
      },
      "source": [
        "Here is a quick test to check if your code is working, at least for a simple case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAEL5fDSW18b"
      },
      "source": [
        "w1 = tf.ones((1, 10))\n",
        "b1 = tf.zeros(10)\n",
        "w2 = tf.ones((10, 1))\n",
        "b2 = tf.ones(1)\n",
        "x = 2 * tf.ones((1, 1))\n",
        "yhat = nn_2layers(x, [w1, b1, w2, b2], linear, linear)\n",
        "assert yhat.numpy().squeeze() == 21."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG9xLmTVZVku"
      },
      "source": [
        "Now let's take a quick look at how TensorFlow can compute derivatives/gradients in a few simpler cases as we build up to using it to fit a neural network.\n",
        "\n",
        "First let's do a simple function where we know the answer -- let's compute the derivative of $f(x) = x^2 + 5x$.\n",
        "\n",
        "Do do this we the `tf.GradientTape()` object as a \"context manager\" where TensorFlow will track the computational graph of the calculation of $f(x)$ during the \"forward pass\". Then we can ask the tape to for the derivate of one of our values with respect to another, and it will be able to use the information that it stored to compute the \"backward pass\" to produce the desired result.\n",
        "\n",
        "We will compute the gradient at $x = 1$, where we know the results should be:\n",
        "$$\n",
        "\\begin{align}\n",
        "f(1) &=  1^2 + 5 = 6 \\\\\n",
        "f'(x) &= 2x+5 \\\\\n",
        "f'(1) &= 2\\cdot 1 + 5 = 7\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRFyaC0UZTmU"
      },
      "source": [
        "x = tf.Variable(1.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  f_x = x**2 + 5*x\n",
        "print(f_x)\n",
        "print(tape.gradient(f_x, x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uxUuSVkmg2U"
      },
      "source": [
        "Now let's try something a little more challenging. Consider the function\n",
        "$$\n",
        "f(x, y) = \\frac{e^x\\cdot \\sin{\\sqrt{y + x^3}}}{\\tanh{y}}\n",
        "$$\n",
        "Compute its gradient:\n",
        "$$\n",
        "\\Big[\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\Big]\n",
        "$$\n",
        "when $x = 1$, $y = 2$.\n",
        "\n",
        "Look at the docs for `GradientTape.gradient` to figure out how to compute multiple derivatives at once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoVyu6BXmaF0"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYMrGuElowbD"
      },
      "source": [
        "Now that you know how to compute gradients of any function, it's time to put this to use fitting a neural net. First, here is some synthetic nonlinear regression data to which we can fit our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGQSviJORkp5"
      },
      "source": [
        "n = 200\n",
        "x = np.random.uniform(low=-1, high=1, size=n)\n",
        "eps = 0.2 * np.random.randn(n)\n",
        "y = 4 * x * np.sin(4 * x) + eps\n",
        "\n",
        "x, y = x[:, np.newaxis].astype(np.float32), y[:, np.newaxis].astype(np.float32)\n",
        "\n",
        "plt.scatter(x, y, s=10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqZTsTfjpMbB"
      },
      "source": [
        "TensorFlow helps us compute gradients of functions. When fitting a neural net via gradient descent, the function that we want to take the gradient of is the loss function. So start by writting a function that compute the loss that we will use for our regression problem: mean squared error.\n",
        "\n",
        "If you need to take an average (hint: you should probably be taking an average), checkout out the `tf.reduce_mean` fuction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2whoJ_K2Rm7"
      },
      "source": [
        "def mse(y_true, y_pred):\n",
        "  '''\n",
        "  y_true: array of target values, shape: (batch size, 1)\n",
        "  y_pred: array of predicted values, shape: (batch size, 1)\n",
        "  '''\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x1eJgx1puiJ"
      },
      "source": [
        "Now let's write a function that does a single step of gradient descent. It should:\n",
        "1. use a gradient tape to track computation\n",
        "2. use `nn_2layers` to compute the prediction from the inputs\n",
        "3. use the `mse` function to compute the loss from the predictions and the targets\n",
        "4. use the gradient tape to compute the gradients\n",
        "5. use the gradients and the step size to update the parameters\n",
        "\n",
        "For updating the variable values, use the `assign()` method on any `Variable` rather than the usual Python assignment operator (`=`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tROIynfPrGDy"
      },
      "source": [
        "def grad_step(x, y, params, f_hidden, f_out, step_size):\n",
        "  '''\n",
        "  updates `params` for a single step of gradient descent\n",
        "\n",
        "  x: array of inputs, shape: (batch size, input dim)\n",
        "  y: array of targets, shape: (batch size, 1)\n",
        "  params: list of parameters (tf.Variables), [w1, b1, w2, b2]]\n",
        "  f_hidden: activation function for the hidden layer\n",
        "  f_out: activation function for the output layer\n",
        "  step_size: gradient descent step size parameter\n",
        "\n",
        "  returns: the loss\n",
        "  '''\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjP5hIyzr7kU"
      },
      "source": [
        "Finally, let's put everything together to write a function that trains a neural net via gradient descent. Follow the following steps:\n",
        "1. Generate random initial values for the parameters (check out `tf.random.uniform` and pick reasonable bounds)\n",
        "2. Make `tf.Variable` values with these initial values\n",
        "3. Use a loop to call your `grad_step` function repeatedly and save the loss values in a list as you go so you can return it at the end\n",
        "\n",
        "We will assume that the input and targets are both 1-d, so the only thing we need to specify to know what shapes to use for the parameters is the number of units in the hidden layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShUMw8_8syFo"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def nn_grad_desc(x, y, n_hidden, f_hidden, f_out, step_size, n_iters):\n",
        "  '''\n",
        "  fit a two-layer neural net via gradient descent\n",
        "  '''\n",
        "  w1 = tf.random.uniform((1, n_hidden), minval=-1, maxval=1)\n",
        "  b1 = tf.random.uniform((n_hidden,), minval=-1, maxval=1)\n",
        "\n",
        "  w2 = tf.random.uniform((n_hidden, 1), minval=-1, maxval=1)\n",
        "  b2 = tf.random.uniform((1,), minval=-1, maxval=1)\n",
        "\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRZC3tBj0snH"
      },
      "source": [
        "Now put all of your hard work to use! Call your function to fit the neural net and then plot your neural nets predictions against the data.\n",
        "\n",
        "Some notes:\n",
        "- Use the `tf.linspace` function to get a set of evenly spaced values to predict on for plotting\n",
        "- Resuse your `nn_2layers` function to make predictions using the fitted parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvLM6zYn1RVM"
      },
      "source": [
        "# your code + plots here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}